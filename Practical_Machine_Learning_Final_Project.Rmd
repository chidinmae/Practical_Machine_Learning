---
title: "Practical Machine Learning Final Project"
author: Chidinma Egbukichi
date: March 6, 2016
output: github_document
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("~/Documents/Coursera/Practical_Machine_Learning/Practical_Machine_Learning/fin_proj.RData")
```
## The Assignment
 
> ## From the instructor:
 
> One thing that people regularly do is quantify how much
> of a particular activity they do, but they rarely
> quantify how well they do it. In this project, your goal
> will be to use data from accelerometers on the belt,
> forearm, arm, and dumbell of 6 participants. They were
> asked to perform barbell lifts correctly and incorrectly
> in 5 different ways.
> 
> The goal of your project is to predict the manner in
> which they did the exercise. This is the "classe"
> variable in the training set. You may use any of the
> other variables to predict with.
 
## Cleaning the Data
 
```
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
View(training)
```
 
Looking at the training data, the first 7 variables will
not be useful for prediction so they are dropped from the
training set.
```
subset_training <- subset(training, select = -c(X:num_window))
```
 
Further, there are many variables that are mainly populated with NA
orblank values. Those will be removed from the training set as well.
```
subset_training[subset_training==""] <- NA
training_NAs <- is.na(subset_training)
bad <- which(colSums(training_NAs) > 19000)
subset_training <- subset_training[,-bad]
```
 
Lastly, check for row completion.
```
complete <- complete.cases(subset_training)
subset_training <- subset_training[complete,]
```
 
After cleaning there are 52 predictors, 1 Response and 19,622 observations
```{r}
dim(subset_training)
```
 
## Classification Method and Cross Validation
 
I used the boosting method of classification because, as discussed in lecture, it is one of the top-performing algorithms for prediction (along with random forest). Boosting combines many weak predictors, as is likely the case with data, to create stronger composite predictors.
 
I did not use random forest because of the increased danger of overfitting.
 
Because this a classification problem (categorical responses), regression methods are unsuitable.
 
For cross validation, I used the trControl function in caret and selected a k-folds cross validation technique. I chose to use 10 folds because there were a large enough number of observations to account for the bias and variance trade-off.
 
```
library("caret")
fitControl <- trainControl(method="cv",number=10)
model_gbm <- train(classe~.,data=subset_training,trControl=fitControl,method="gbm",verbose=FALSE)
```
 
## Model and Expected Out of Sample Error
```{r, echo=FALSE}
model_gbm
```

The best sample had a 96.4% accuracy on the training set with 150 trees and an interaction 
depth of 3. As always, I expect the in sample error to be lower than the out of sample error. 
I predict 5% out of sample error in the test set.
 
## Predictions
```
predict_gbm <- predict(model_gbm, new_testing)
```
```{r}
predict_gbm
```

